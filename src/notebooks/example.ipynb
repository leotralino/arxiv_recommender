{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import logging\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# do logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'b'.join(['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_papers(categories: list[str]=[\"cs.AI\"], days: int=1, max_results=100):\n",
    "    \"\"\"\n",
    "    Fetches papers from specific categories within a time window.\n",
    "    \"\"\"\n",
    "    # 1. Build Query (e.g., \"cat:cs.LG OR cat:cs.AI\")\n",
    "    query = \" OR \".join([f\"cat:{c}\" for c in categories])\n",
    "    \n",
    "    client = arxiv.Client(\n",
    "        page_size=100,\n",
    "        delay_seconds=3.0, # Be nice to ArXiv servers\n",
    "        num_retries=3\n",
    "    )\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    # 2. Time Window\n",
    "    threshold = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=days)\n",
    "    \n",
    "    results = []\n",
    "    for result in client.results(search):\n",
    "        # ArXiv results are sorted by date, so we can break early\n",
    "        if result.published < threshold:\n",
    "            break\n",
    "            \n",
    "        results.append({\n",
    "            \"id\": result.entry_id.split('/')[-1],\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"abstract\": result.summary.replace(\"\\n\", \" \"),\n",
    "            \"published\": result.published,\n",
    "            \"primary_category\": result.primary_category,\n",
    "            \"url\": result.pdf_url,\n",
    "        })\n",
    "        \n",
    "    since_time = threshold.strftime('%Y-%m-%d %H:%M')\n",
    "    print(\n",
    "        f\"Fetched {len(results)} new papers since {since_time}\"\n",
    "    )\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_recent_papers(max_results=1000)\n",
    "df['combined_text'] = \"Title: \" + df['title'] + \" Abstract: \" + df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_interest_embedding = model.encode(\"I am interested in Retrieval-augmented generation (RAG).\")\n",
    "content_embedding = model.encode(df['combined_text'].tolist())\n",
    "\n",
    "print(np.shape(content_embedding))\n",
    "\n",
    "cosine_sim = np.dot(content_embedding, my_interest_embedding) / (np.linalg.norm(content_embedding, axis=1) * np.linalg.norm(my_interest_embedding))\n",
    "top_k_indices = np.argsort(-cosine_sim)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = df.iloc[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "def get_authors(author_list: list[str]):\n",
    "    \"\"\"\n",
    "    Formats the author list for LLM input, truncating if too long.\n",
    "    \"\"\"\n",
    "    num_authors = len(author_list)\n",
    "\n",
    "    if num_authors <= 12:\n",
    "        author_string = \", \".join(author_list)\n",
    "    else:\n",
    "        first_part = author_list[:10]\n",
    "        last_part = author_list[-2:]\n",
    "        author_string = f\"{', '.join(first_part)} ... {', '.join(last_part)}\"\n",
    "    \n",
    "    return author_string\n",
    "\n",
    "def parse_llm_output(response_text: str):\n",
    "    \"\"\"\n",
    "    Sometimes LLM does not give list of JSON, so return {\"paper\": [...]} instead.\n",
    "    Then parse it accordingly.\n",
    "    \"\"\"\n",
    "    data = json.loads(response_text)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key in ['papers', 'results', 'recommendations']:\n",
    "            if key in data:\n",
    "                return data[key]\n",
    "    return []\n",
    "\n",
    "def fine_rank_with_llm(top_papers_df: pd.DataFrame, user_interest:str=\"RAG\"):\n",
    "    \"\"\"\n",
    "    Uses a local LLM to give a qualitative 'Quality Score' to the top candidates.\n",
    "    \"\"\"\n",
    "    top_papers_df = top_papers_df.copy()\n",
    "    top_papers_df['cleaned_authors'] = top_papers_df['authors'].apply(get_authors)\n",
    "    all_papers_json = top_papers_df[['id', 'title', 'cleaned_authors', 'abstract']].to_dict(orient='records')\n",
    "\n",
    "    logger.info(f\"Number of papers to process: {len(all_papers_json)}\")\n",
    "    logger.info(f\"Sample input paper JSON: {all_papers_json[0]}\")\n",
    "\n",
    "    llm_ranking_prompt = f\"\"\"\n",
    "    ## Task\n",
    "\n",
    "    You are an expert AI and Physics PhD Researcher. \n",
    "    Given a list of papers with their titles, authors, and abstracts, you should help me decide which research papers to read based on my specific interest.\n",
    "    These papers are posted on arXiv today.\n",
    "    Your output should be top-5 results, stored in JSON format.\n",
    "\n",
    "    To select best papers, you should consider relevancy to my interest, novelty, and soundness of methodology. Use the author list to gauge credibility if needed.\n",
    "    \n",
    "\n",
    "    ## My Interest\n",
    "    My research interest topics: \"{user_interest}\"\n",
    "\n",
    "    \n",
    "    ## Input paper as JSON array\n",
    "    {all_papers_json}\n",
    "\n",
    "    ## Output\n",
    "\n",
    "    In the \"papers\" field, provide a list of JSON response with top-5 papers, with the following fields:\n",
    "    - \"id\": The ArXiv ID of the paper.\n",
    "    - \"reasoning\": A one-sentence explanation of why this paper is worth reading or not.\n",
    "\n",
    "    The output format should look like this:\n",
    "    {{\n",
    "        \"papers\": \n",
    "        [\n",
    "            {{\"id\": \"paper_id_1\", \"reasoning\": \"Reasoning for paper 1...\"}},\n",
    "            {{\"id\": \"paper_id_2\", \"reasoning\": \"Reasoning for paper 2...\"}},\n",
    "            ...\n",
    "            {{\"id\": \"paper_id_5\", \"reasoning\": \"Reasoning for paper 5...\"}}\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model=\"llama3:8b\", \n",
    "        prompt=llm_ranking_prompt,\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0}\n",
    "    )\n",
    "    \n",
    "    llm_output = parse_llm_output(response[\"response\"])\n",
    "    logger.info(f\"LLM Output: {llm_output}\")\n",
    "    \n",
    "    refined_results = []\n",
    "    for paper_analysis in llm_output:\n",
    "        paper_id = paper_analysis['id']\n",
    "        matching_paper = top_papers_df[top_papers_df['id'] == paper_id].iloc[0]\n",
    "        \n",
    "        refined_results.append({\n",
    "            \"id\": paper_id,\n",
    "            \"url\": matching_paper[\"url\"],\n",
    "            \"title\": matching_paper[\"title\"],\n",
    "            \"authors\": matching_paper[\"cleaned_authors\"],\n",
    "            \"abstract\": matching_paper[\"abstract\"],\n",
    "            \"reasoning\": paper_analysis[\"reasoning\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(refined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = fine_rank_with_llm(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
